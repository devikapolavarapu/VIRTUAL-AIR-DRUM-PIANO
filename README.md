ğŸµ Virtual Air Drum & Piano

A real-time gesture-controlled virtual musical instrument that allows users to play drums and piano in mid-air using only a webcam.
The project uses computer vision and hand-gesture recognition to detect fingertip movements and trigger corresponding sounds with low latency.

This system demonstrates how Humanâ€“Computer Interaction (HCI) can be built using vision-based input instead of physical hardware.

ğŸš€ Project Overview

Virtual Air Drum & Piano tracks hand landmarks in real time using MediaPipe, processes fingertip motion using OpenCV, and triggers instrument sounds using pygame.

The same gesture-detection pipeline supports two instrument modes:

ğŸ¥ Drum Mode â€“ Percussion-based virtual drum pads

ğŸ¹ Piano Mode â€“ Musical note-based virtual piano keys

The project is modular, scalable, and designed with extensibility in mind.

âœ¨ Key Features

ğŸ¥ Live webcam-based hand tracking

âœ‹ Fingertip detection using MediaPipe (21 landmarks)

âš¡ Velocity-based downward tap detection (noise-resistant)

ğŸ§­ Spatial zone mapping for gesture interpretation

ğŸ”Š Low-latency real-time audio playback

ğŸ¹ Dual instrument support (Drum & Piano modes)

ğŸ§  Clean cooldown & debouncing logic

âŒ¨ï¸ Press q to exit safely

ğŸ¥ Drum Mode

Drum Mode maps the screen into five vertical percussion zones:

Zone	Instrument
1	Kick
2	Snare
3	Hi-Hat
4	Tom
5	Clap

How it works:

A fast downward fingertip tap is detected

The X-axis position determines the drum zone

The corresponding drum sound is triggered

Run:

python main.py

ğŸ¹ Piano Mode

Piano Mode extends the same gesture pipeline to a melodic instrument.

The screen is divided into seven piano keys:

C   D   E   F   G   A   B


Each key corresponds to a musical note frequency.
A fingertip tap on a key triggers the respective note.

This mode demonstrates project extensibility without rewriting core logic.

Run:

python piano_mode.py

ğŸ› ï¸ Tech Stack
Category	Technologies
Programming Language	Python 3.10+
Computer Vision	OpenCV
Hand Tracking	MediaPipe
Audio Engine	pygame
Numerical Computing	NumPy
Version Control	Git & GitHub
ğŸ§© System Architecture (High-Level)

Webcam captures live video frames

MediaPipe extracts hand landmarks

Fingertip coordinates are tracked per frame

Downward velocity is calculated to detect taps

X-axis position maps gesture to an instrument zone

Corresponding sound is played in real time

â–¶ï¸ How to Run the Project
ğŸ”¹ Prerequisites

Python 3.10 or higher

Functional webcam

Windows / macOS / Linux

ğŸ”¹ Install Dependencies
pip install opencv-python mediapipe pygame numpy

ğŸ”¹ Run Drum Mode
python main.py

ğŸ”¹ Run Piano Mode
python piano_mode.py

ğŸ® How to Use

Place your hand in front of the webcam

Use your index finger

Perform a fast downward tap

Move left â†” right to select instruments or notes

ğŸ‘©â€ğŸ’» Author

Devika Polavarapu
B.Tech â€“ Information Technology

Interests:

Computer Vision

AI & Humanâ€“Computer Interaction

Real-world software systems

ğŸ”— GitHub: https://github.com/devikapolavarapu

Press q to exit
